{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-14 23:32:39.413273: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer, InputExample, losses, models\n",
    "from torch.utils.data import DataLoader\n",
    "from collections import defaultdict\n",
    "from scipy import spatial\n",
    "import random\n",
    "def def_value():\n",
    "    return []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "word_embedding_model = models.Transformer('bert-base-uncased', max_seq_length=256)\n",
    "pooling_model = models.Pooling(word_embedding_model.get_word_embedding_dimension())\n",
    "\n",
    "model = SentenceTransformer(modules=[word_embedding_model, pooling_model])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SimANS Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batches(model,docs,relevance_dict,batch_size):\n",
    "    embeddings = model.encode(docs)\n",
    "    input_batches = []\n",
    "    for id in relevance_dict.keys():\n",
    "        if id == 5:\n",
    "            break\n",
    "        if not relevance_dict[id] == []:\n",
    "            cosine_sims = [1 - spatial.distance.cosine(embedding, embeddings[id]) for embedding in embeddings]\n",
    "            positive = random.choice(relevance_dict[id])\n",
    "            negatives = []\n",
    "            sim_ranked_indexes = [index for _, index in sorted(zip(cosine_sims, range(len(cosine_sims))))]\n",
    "            positive_index = sim_ranked_indexes.index(positive)\n",
    "            index = 1\n",
    "            while len(negatives) < batch_size-1:\n",
    "                if positive_index-index > 0 and positive_index-index not in relevance_dict[id]:\n",
    "                    if len(negatives) < batch_size-1:\n",
    "                        negatives.append(positive_index-index)\n",
    "                if positive_index+index < len(docs) and positive_index+index not in relevance_dict[id]:\n",
    "                    if len(negatives) < batch_size-1:\n",
    "                        negatives.append(positive_index+index)\n",
    "                \n",
    "                index += 1\n",
    "            \n",
    "            batch = [sim_ranked_indexes[neg] for neg in negatives]\n",
    "            labels = [0.0 for _ in range(batch_size-1)]\n",
    "            batch.append(positive)\n",
    "            labels.append(1.0)\n",
    "\n",
    "            zipped = list(zip(batch, labels))\n",
    "            random.shuffle(zipped)\n",
    "            batch, labels = zip(*zipped)\n",
    "\n",
    "            input_batch = create_input_batch(batch,labels,docs,id)\n",
    "            input_batches.append(input_batch)\n",
    "\n",
    "    return [pair for batch in input_batches for pair in batch]\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_input_batch(batches,batch_labels,docs,id):\n",
    "    input_batch = []\n",
    "    for i,doc in enumerate(batches):\n",
    "        input_batch.append(InputExample(texts=[docs[id],docs[doc]],label=batch_labels[i]))\n",
    "    return input_batch\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Pre-Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def prepare_data(filename):\n",
    "\n",
    "    with open(filename) as f:\n",
    "        lines = f.readlines()\n",
    "\n",
    "    docs = []\n",
    "    relevances_list = []\n",
    "    for index, line in enumerate(lines):\n",
    "        previous_line = index-1\n",
    "        if lines[previous_line][:2] == \".W\":\n",
    "            sentence = \"\"\n",
    "            while not previous_line+1 == len(lines) and not lines[previous_line+1][:1] == \".\":\n",
    "                previous_line += 1\n",
    "                sentence += \" \" + lines[previous_line]\n",
    "            docs.append(sentence)\n",
    "        elif lines[previous_line][:2] == \".X\":\n",
    "            while not previous_line+1 == len(lines) and not lines[previous_line+1][0] == \".\":\n",
    "                previous_line += 1\n",
    "                relevances_list.append(lines[previous_line])\n",
    "\n",
    "    relevance_dict = defaultdict(def_value)\n",
    "    for relevance in relevances_list:\n",
    "        metadata = relevance.replace(\"\\n\",\"\").split(\"\\t\")\n",
    "        if not metadata[0] == metadata[2]:\n",
    "            relevance_dict[int(metadata[2])-1].append(int(metadata[0])-1)\n",
    "\n",
    "    return docs, relevance_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs, relevance_dict = prepare_data(\"CISI.ALL\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fine_tune_BERT(batch_size,epochs,calc_negatives_per_epoch):\n",
    "    train_loss = losses.CosineSimilarityLoss(model)\n",
    "    for epoch in range(epochs):\n",
    "        print(\"epoch\", epoch+1)\n",
    "        if epoch % calc_negatives_per_epoch == 0:\n",
    "            print(\"calculating Batches\")\n",
    "            input_batches = get_batches(model,docs,relevance_dict,batch_size)\n",
    "        print(\"Training Model\")\n",
    "        train_dataloader = DataLoader(input_batches, shuffle=False, batch_size=batch_size)\n",
    "        model.fit(train_objectives=[(train_dataloader, train_loss)], epochs=1, warmup_steps=100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fine_tune_BERT(batch_size=20,epochs=8,calc_negatives_per_epoch=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retriever(query,corpus_embedded,k):\n",
    "    query_embedding = model.encode(query)\n",
    "    cosine_sims = [1 - spatial.distance.cosine(embedding, query_embedding) for embedding in corpus_embedded]\n",
    "    sim_ranked_indexes = [index for _, index in sorted(zip(cosine_sims, range(len(cosine_sims))))]\n",
    "    print(sim_ranked_indexes)\n",
    "    return sim_ranked_indexes[:k]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_embedded = model.encode(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "[1295, 1287, 1118, 1288, 54, 1289, 1085, 442, 1165, 1300, 1294, 1319, 1278, 1286, 1281, 429, 1099, 1305, 680, 1301]\n"
     ]
    }
   ],
   "source": [
    "top_k = retriever(docs[0],corpus_embedded,20)\n",
    "print(top_k)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "397704579725e15f5c7cb49fe5f0341eb7531c82d19f2c29d197e8b64ab5776b"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
