{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-15 18:06:33.923088: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer, InputExample, losses, models\n",
    "from torch.utils.data import DataLoader\n",
    "from collections import defaultdict\n",
    "from scipy import spatial\n",
    "import random\n",
    "import re\n",
    "from scipy import spatial\n",
    "stopwords = [\"ourselves\", \"hers\", \"between\", \"yourself\", \"but\", \"again\", \n",
    "            \"there\", \"about\", \"once\", \"during\", \"out\", \"very\", \"having\", \n",
    "            \"with\", \"they\", \"own\", \"an\", \"be\", \"some\", \"for\", \"do\", \"its\", \n",
    "            \"yours\", \"such\", \"into\", \"of\", \"most\", \"itself\", \"other\", \"off\", \n",
    "            \"is\", \"s\", \"am\", \"or\", \"who\", \"as\", \"from\", \"him\", \"each\", \"the\", \n",
    "            \"themselves\", \"until\", \"below\", \"are\", \"we\", \"these\", \"your\", \"his\", \n",
    "            \"through\", \"don\", \"nor\", \"me\", \"were\", \"her\", \"more\", \"himself\", \"this\", \n",
    "            \"down\", \"should\", \"our\", \"their\", \"while\", \"above\", \"both\", \"up\", \"to\", \n",
    "            \"ours\", \"had\", \"she\", \"all\", \"no\", \"when\", \"at\", \"any\", \"before\", \"them\", \n",
    "            \"same\", \"and\", \"been\", \"have\", \"in\", \"will\", \"on\", \"does\", \"yourselves\", \n",
    "            \"then\", \"that\", \"because\", \"what\", \"over\", \"why\", \"so\", \"can\", \"did\", \"not\", \n",
    "            \"now\", \"under\", \"he\", \"you\", \"herself\", \"has\", \"just\", \"where\", \"too\", \"only\", \n",
    "            \"myself\", \"which\", \"those\", \"i\", \"after\", \"few\", \"whom\", \"t\", \"being\", \"if\", \n",
    "            \"theirs\", \"my\", \"against\", \"a\", \"by\", \"doing\", \"it\", \"how\", \"further\", \"was\", \n",
    "            \"here\", \"than\"]\n",
    "def def_value():\n",
    "    return []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "BERT_model = SentenceTransformer('all-MiniLM-L6-v2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SimANS Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batches(BERT_model,docs,relevance_dict,batch_size):\n",
    "    embeddings = BERT_model.encode(docs)\n",
    "    input_batches = []\n",
    "    for id in relevance_dict.keys():\n",
    "        if id == 5:\n",
    "            break\n",
    "        if not relevance_dict[id] == []:\n",
    "            cosine_sims = [1 - spatial.distance.cosine(embedding, embeddings[id]) for embedding in embeddings]\n",
    "            positive = random.choice(relevance_dict[id])\n",
    "            negatives = []\n",
    "            sim_ranked_indexes = [index for _, index in sorted(zip(cosine_sims, range(len(cosine_sims))))]\n",
    "            positive_index = sim_ranked_indexes.index(positive)\n",
    "            index = 1\n",
    "            while len(negatives) < batch_size-1:\n",
    "                if positive_index-index > 0 and positive_index-index not in relevance_dict[id]:\n",
    "                    if len(negatives) < batch_size-1:\n",
    "                        negatives.append(positive_index-index)\n",
    "                if positive_index+index < len(docs) and positive_index+index not in relevance_dict[id]:\n",
    "                    if len(negatives) < batch_size-1:\n",
    "                        negatives.append(positive_index+index)\n",
    "                \n",
    "                index += 1\n",
    "            \n",
    "            batch = [sim_ranked_indexes[neg] for neg in negatives]\n",
    "            labels = [0.0 for _ in range(batch_size-1)]\n",
    "            batch.append(positive)\n",
    "            labels.append(1.0)\n",
    "\n",
    "            zipped = list(zip(batch, labels))\n",
    "            random.shuffle(zipped)\n",
    "            batch, labels = zip(*zipped)\n",
    "\n",
    "            input_batch = create_input_batch(batch,labels,docs,id)\n",
    "            input_batches.append(input_batch)\n",
    "\n",
    "    return [pair for batch in input_batches for pair in batch]\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_input_batch(batches,batch_labels,docs,id):\n",
    "    input_batch = []\n",
    "    for i,doc in enumerate(batches):\n",
    "        input_batch.append(InputExample(texts=[docs[id],docs[doc]],label=batch_labels[i]))\n",
    "    return input_batch\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Pre-Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def prepare_data(filename):\n",
    "\n",
    "    with open(filename) as f:\n",
    "        lines = f.readlines()\n",
    "\n",
    "    docs = []\n",
    "    relevances_list = []\n",
    "    for index, line in enumerate(lines):\n",
    "        previous_line = index-1\n",
    "        if lines[previous_line][:2] == \".W\":\n",
    "            sentence = \"\"\n",
    "            while not previous_line+1 == len(lines) and not lines[previous_line+1][:1] == \".\":\n",
    "                previous_line += 1\n",
    "                sentence += \" \" + lines[previous_line]\n",
    "            docs.append(sentence)\n",
    "        elif lines[previous_line][:2] == \".X\":\n",
    "            while not previous_line+1 == len(lines) and not lines[previous_line+1][0] == \".\":\n",
    "                previous_line += 1\n",
    "                relevances_list.append(lines[previous_line])\n",
    "\n",
    "    relevance_dict = defaultdict(def_value)\n",
    "    for relevance in relevances_list:\n",
    "        metadata = relevance.replace(\"\\n\",\"\").split(\"\\t\")\n",
    "        if not metadata[0] == metadata[2]:\n",
    "            relevance_dict[int(metadata[2])-1].append(int(metadata[0])-1)\n",
    "\n",
    "    return docs, relevance_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs, relevance_dict = prepare_data(\"CISI.ALL\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fine_tune_BERT(BERT_model,docs,relevance_dict,batch_size,epochs,calc_negatives_per_epoch):\n",
    "    train_loss = losses.CosineSimilarityLoss(BERT_model)\n",
    "    for epoch in range(epochs):\n",
    "        print(\"epoch\", epoch+1)\n",
    "        if epoch % calc_negatives_per_epoch == 0:\n",
    "            print(\"calculating Batches\")\n",
    "            input_batches = get_batches(BERT_model,docs,relevance_dict,batch_size)\n",
    "        print(\"Training Model\")\n",
    "        train_dataloader = DataLoader(input_batches, shuffle=False, batch_size=batch_size)\n",
    "        BERT_model.fit(train_objectives=[(train_dataloader, train_loss)], epochs=1, warmup_steps=100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1\n",
      "calculating Batches\n",
      "Training Model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "Iteration: 100%|██████████| 5/5 [00:22<00:00,  4.53s/it]\n",
      "Epoch: 100%|██████████| 1/1 [00:22<00:00, 22.68s/it]\n"
     ]
    }
   ],
   "source": [
    "fine_tune_BERT(BERT_model,docs,relevance_dict,batch_size=20,epochs=1,calc_negatives_per_epoch=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_top_k(BERT_model,query,corpus_embedded,k):\n",
    "    query_embedding = BERT_model.encode(query)\n",
    "    cosine_sims = [1 - spatial.distance.cosine(embedding, query_embedding) for embedding in corpus_embedded]\n",
    "    sim_ranked_indexes = [index for _, index in sorted(zip(cosine_sims, range(len(cosine_sims))),reverse=True)]\n",
    "    return sim_ranked_indexes[:k]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_embedded = BERT_model.encode(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "success_rate 0.31303500346829805\n"
     ]
    }
   ],
   "source": [
    "success_rate = 0\n",
    "for query in range(len(docs)):\n",
    "    if not len(relevance_dict[query]) == 0:\n",
    "        top_k = retrieve_top_k(BERT_model,docs[query],corpus_embedded,100)\n",
    "        retreived = 0\n",
    "        for positive in relevance_dict[query]:\n",
    "            if positive in top_k:\n",
    "                retreived += 1\n",
    "        success_rate += retreived/len(relevance_dict[query])\n",
    "print(\"success_rate\", success_rate/len(docs))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ColBERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text,is_query,max_query_lenght=0):\n",
    "    clean_text = re.sub(r'[^\\w]', ' ', text)\n",
    "    clean_text_list = clean_text.split()\n",
    "    clean_text_list_no_stopwords = [token for token in clean_text_list if token not in stopwords]\n",
    "    if is_query:\n",
    "        return clean_text_list_no_stopwords[:max_query_lenght]\n",
    "    return clean_text_list_no_stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_corpus_token_embeddings(model,docs):\n",
    "    corpus_token_embeddings = []\n",
    "    for doc in docs:\n",
    "        doc_embeddings = get_text_token_embedding(model,doc)\n",
    "        corpus_token_embeddings.append(doc_embeddings)\n",
    "    return corpus_token_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_text_token_embedding(model,text,is_query=False,max_query_lenght=0):\n",
    "    doc_tokens = preprocess_text(text,is_query,max_query_lenght)\n",
    "    doc_embeddings = [model.encode(token) for token in doc_tokens]\n",
    "    return doc_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ColBERT_similarity(query_embeddings,doc_embeddings):\n",
    "    doc_tokens_tree = spatial.KDTree(doc_embeddings)\n",
    "    total_sim = 0\n",
    "    for query_embedding in query_embeddings:\n",
    "        closest_embedding_idx = doc_tokens_tree.query(query_embedding)[1]\n",
    "        max_cosine_sim = 1 - spatial.distance.cosine(doc_embeddings[closest_embedding_idx], query_embedding)\n",
    "        total_sim += max_cosine_sim\n",
    "\n",
    "    return total_sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def re_rank(query,top_k_token_embeddings,top_k_idx,max_query_tokens):\n",
    "    query_embeddings = get_text_token_embedding(BERT_model,query,True,max_query_tokens)\n",
    "    similarities = []\n",
    "    for token_embeddings in top_k_token_embeddings:\n",
    "        similarities.append(ColBERT_similarity(query_embeddings,token_embeddings))\n",
    "    sim_ranked_indexes = [index for _, index in sorted(zip(similarities, top_k_idx),reverse=True)]\n",
    "    return sim_ranked_indexes\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_doc_embeddings = get_corpus_token_embeddings(BERT_model,docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for query_index in range(len(corpus_doc_embeddings)):\n",
    "\n",
    "    false = 0\n",
    "    false_count = 0\n",
    "    true = 0\n",
    "    true_count = 0\n",
    "\n",
    "    query_embeddings = get_text_token_embedding(BERT_model,docs[query_index],True,250)\n",
    "\n",
    "    for doc_idx in range(len(corpus_doc_embeddings)):\n",
    "        doc_embeddings = corpus_doc_embeddings[doc_idx]\n",
    "        total_sim = ColBERT_similarity(query_embeddings,doc_embeddings)\n",
    "        if doc_idx == query_index:\n",
    "            pass\n",
    "        elif doc_idx in relevance_dict[query_index]:\n",
    "            true += total_sim\n",
    "            true_count += 1\n",
    "        else:\n",
    "            false += total_sim\n",
    "            false_count += 1\n",
    "\n",
    "    false = false / false_count\n",
    "    if true_count > 0:\n",
    "        true = true / true_count\n",
    "\n",
    "    print(\"False\", false)\n",
    "    print(\"True\", true)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Full Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "def document_retrieval_model(query,corpus_embedded,corpus_token_embedded,BERT_model):\n",
    "\n",
    "    ## HYPER PARAMS ##\n",
    "    k = 100\n",
    "    max_query_tokens = 250\n",
    "\n",
    "    top_k_idx = retrieve_top_k(BERT_model,query,corpus_embedded,k)\n",
    "    top_k_token_embeddings = [corpus_token_embedded[idx] for idx in top_k_idx]\n",
    "    top_k_re_ranked = re_rank(query,top_k_token_embeddings,top_k_idx,max_query_tokens)\n",
    "    \n",
    "    return top_k_re_ranked"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Full Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_embedded = BERT_model.encode(docs)\n",
    "corpus_token_embedded = get_corpus_token_embeddings(BERT_model,docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load new dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(docs_file,query_file,relevance_file):\n",
    "\n",
    "    with open(docs_file) as f:\n",
    "        doc_lines = f.readlines()\n",
    "\n",
    "    docs = []\n",
    "    for index, line in enumerate(doc_lines):\n",
    "        previous_line = index-1\n",
    "        if doc_lines[previous_line][:2] == \".W\":\n",
    "            sentence = \"\"\n",
    "            while not previous_line+1 == len(doc_lines) and not doc_lines[previous_line+1][:1] == \".\":\n",
    "                previous_line += 1\n",
    "                sentence += \" \" + doc_lines[previous_line]\n",
    "            docs.append(sentence.replace(\"\\n\",\"\"))\n",
    "    \n",
    "    with open(query_file) as f:\n",
    "        query_lines = f.readlines()\n",
    "    \n",
    "    queries = []\n",
    "    for index, line in enumerate(query_lines):\n",
    "        previous_line = index-1\n",
    "        if query_lines[previous_line][:2] == \".W\":\n",
    "            sentence = \"\"\n",
    "            while not previous_line+1 == len(query_lines) and not query_lines[previous_line+1][:1] == \".\":\n",
    "                previous_line += 1\n",
    "                sentence += \" \" + query_lines[previous_line]\n",
    "            queries.append(sentence.replace(\"\\n\",\"\"))\n",
    "    \n",
    "    with open(relevance_file) as f:\n",
    "        relevance_lines = f.readlines()\n",
    "\n",
    "    relevance_dict = defaultdict(def_value)\n",
    "    relevance_dict_rated = {}\n",
    "    for line in relevance_lines:\n",
    "        indexes = line.split()\n",
    "        relevance_dict[int(indexes[0])-1].append(int(indexes[1])-1)\n",
    "        relevance_dict_rated[str(int(indexes[0])-1) + \"-\" + str(int(indexes[1])-1)] = int(indexes[2])\n",
    "\n",
    "\n",
    "    return docs, queries, relevance_dict, relevance_dict_rated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs2, queries2, relevance_dict2, relevance_dict_rated = prepare_data(\"cran/cran.all.1400\",\"cran/cran.qry\",\"cran/cranqrel\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_embedded = BERT_model.encode(docs2)\n",
    "corpus_token_embedded = get_corpus_token_embeddings(BERT_model,docs2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n",
      "-1\n",
      "__\n",
      "__\n",
      "2\n",
      "4\n",
      "__\n",
      "3\n",
      "__\n",
      "__\n",
      "__\n",
      "__\n",
      "__\n",
      "3\n",
      "__\n",
      "__\n",
      "__\n",
      "__\n",
      "__\n",
      "__\n",
      "__\n",
      "__\n",
      "__\n",
      "__\n",
      "__\n",
      "__\n",
      "__\n",
      "__\n",
      "__\n",
      "__\n",
      "__\n",
      "__\n",
      "__\n",
      "__\n",
      "__\n",
      "__\n",
      "__\n",
      "__\n",
      "__\n",
      "__\n",
      "__\n",
      "__\n",
      "__\n",
      "__\n",
      "__\n",
      "__\n",
      "__\n",
      "__\n",
      "__\n",
      "__\n",
      "__\n",
      "__\n",
      "__\n",
      "__\n",
      "__\n",
      "__\n",
      "__\n",
      "__\n",
      "__\n",
      "__\n",
      "__\n",
      "__\n",
      "__\n",
      "__\n",
      "__\n",
      "__\n",
      "__\n",
      "__\n",
      "__\n",
      "__\n",
      "__\n",
      "__\n",
      "__\n",
      "__\n",
      "__\n",
      "__\n",
      "__\n",
      "__\n",
      "__\n",
      "__\n",
      "__\n",
      "__\n",
      "__\n",
      "__\n",
      "__\n",
      "__\n",
      "__\n",
      "__\n",
      "__\n",
      "__\n",
      "__\n",
      "__\n",
      "__\n",
      "__\n",
      "__\n",
      "__\n",
      "__\n",
      "__\n",
      "__\n",
      "__\n",
      "__\n"
     ]
    }
   ],
   "source": [
    "query_idx = 87\n",
    "print(len(relevance_dict2[query_idx]))\n",
    "\n",
    "query = queries2[query_idx]\n",
    "top_k_ranked = document_retrieval_model(query,corpus_embedded,corpus_token_embedded,BERT_model)\n",
    "for idx in top_k_ranked:\n",
    "    if idx in relevance_dict2[query_idx]:\n",
    "        print(relevance_dict_rated[str(query_idx) + \"-\" + str(idx)])\n",
    "    else:\n",
    "        print(\"__\")\n",
    "\n",
    "# print(\"QUESTION:\")\n",
    "# print(queries2[query_idx])\n",
    "# for i in range(3):\n",
    "#     print(\"ANSWER:\")\n",
    "#     print(docs2[top_k_ranked[i]])"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "397704579725e15f5c7cb49fe5f0341eb7531c82d19f2c29d197e8b64ab5776b"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
